{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dataloader import *\n",
    "from training_utils import *\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "\n",
    "trashset = TrashNetDataset(\"trashnet/data/dataset-resized\")\n",
    "valsize = int(len(trashset) * 0.1)\n",
    "trainsize = int(len(trashset) * 0.8)\n",
    "testsize = len(trashset) - valsize - trainsize\n",
    "torch.manual_seed(0) # Ensure dataset is randomly split the same way each time\n",
    "train_dataset, val_dataset, test_dataset = random_split(trashset, [trainsize, valsize, testsize])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading PyTorch .checkpoint model and plotting training metadata. Make sure to set the loadedModel variable to have the same architecture as the initial trained model. For example, if the initial trained model was ResNet152, initialize loadedModel as `loadedModel = resnet152(pretrained=False)`. Also remember to set the model name so it matches the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import *\n",
    "\n",
    "modelName = \"test\"\n",
    "loadedModel = resnet18(pretrained=False)\n",
    "loadedModel.fc = torch.nn.Linear(loadedModel.fc.in_features, len(CLASSES), bias=True)\n",
    "loadedModel.load_state_dict(torch.load(\"models/\" + modelName + \"/weights.checkpoint\"))\n",
    "loadedModel.eval()\n",
    "for param in loadedModel.parameters():\n",
    "    param.requires_grad = False\n",
    "loadedModel.device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    loadedModel.device = 'cuda'\n",
    "loadedModel.to(loadedModel.device)\n",
    "acc = evaluate(loadedModel, test_dataloader)\n",
    "print(\"Accuracy:\", acc)\n",
    "pickle_in = open(\"models/\" + modelName + \"/training_metadata.pickle\",\"rb\")\n",
    "loaded_obj = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "total_loss = loaded_obj[\"total_loss\"]\n",
    "total_acc = loaded_obj[\"total_acc\"]\n",
    "total_learning_rate = loaded_obj[\"total_learning_rate\"]\n",
    "total_batch_size = loaded_obj[\"total_batch_size\"]\n",
    "total_train_full = loaded_obj[\"total_train_full\"]\n",
    "num_epochs = loaded_obj[\"num_epochs\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,3))\n",
    "plt.subplot(141)\n",
    "plt.plot(total_loss)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.subplot(142)\n",
    "plt.plot(total_acc)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.subplot(143)\n",
    "plt.plot(total_batch_size)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Batch Size\")\n",
    "plt.subplot(144)\n",
    "plt.plot(torch.log10(torch.tensor(total_learning_rate)))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Learning Rate (log10)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating ScriptModule to export to C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribedModel = resnet18(pretrained=False)\n",
    "transcribedModel.fc = torch.nn.Linear(transcribedModel.fc.in_features, len(CLASSES), bias=True)\n",
    "transcribedModel.load_state_dict(torch.load(\"models/\" + modelName + \"/weights.checkpoint\"))\n",
    "transcribedModel.eval()\n",
    "for param in transcribedModel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "x, _ = trashset[0]\n",
    "traced_script_module = torch.jit.trace(transcribedModel, x.unsqueeze(0))\n",
    "\n",
    "transcribedModel.device = 'cpu'\n",
    "traced_script_module.device = 'cpu'\n",
    "traced_script_module.save(\"models/\" + modelName + \"/transcripted_model_cpu.pt\")\n",
    "if torch.cuda.is_available():\n",
    "    transcribedModel.device = 'cuda'\n",
    "    traced_script_module.device = 'cuda'\n",
    "transcribedModel.to(transcribedModel.device)\n",
    "traced_script_module.to(traced_script_module.device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    traced_script_module.save(\"models/\" + modelName + \"/transcripted_model_cuda.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify training accuracy (running is optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accScipted = evaluate(traced_script_module, test_dataloader)\n",
    "print(\"Scripted Model Accuracy:\", accScipted)\n",
    "accExpected = evaluate(transcribedModel, test_dataloader)\n",
    "print(\"Expected Model Accuracy:\", accExpected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
